{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/header.png\" alt=\"Logo UCLM-ESII\" align=\"right\">\n",
    "\n",
    "<br><br><br><br>\n",
    "<h2><font color=\"#92002A\" size=4>Trabajo Fin de Grado</font></h2>\n",
    "\n",
    "<h1><font color=\"#6B001F\" size=5>Generaci√≥n autom√°tica de playlist de canciones <br> mediante t√©cnicas de miner√≠a de datos</font></h1>\n",
    "<h2><font color=\"#92002A\" size=3>Parte 8 - Preprocesamiento 1</font></h2>\n",
    "\n",
    "<br>\n",
    "<div style=\"text-align: right\">\n",
    "    <font color=\"#B20033\" size=3><strong>Autor</strong>: <em>Miguel √Ångel Cantero V√≠llora</em></font><br>\n",
    "    <br>\n",
    "    <font color=\"#B20033\" size=3><strong>Directores</strong>: <em>Jos√© Antonio G√°mez Mart√≠n</em></font><br>\n",
    "    <font color=\"#B20033\" size=3><em>Juan √Ångel Aledo S√°nchez</em></font><br>\n",
    "    <br>\n",
    "<font color=\"#B20033\" size=3>Grado en Ingenier√≠a Inform√°tica</font><br>\n",
    "<font color=\"#B20033\" size=2>Escuela Superior de Ingenier√≠a Inform√°tica | Universidad de Castilla-La Mancha</font>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "<a id=\"indice\"></a>\n",
    "<h2><font color=\"#92002A\" size=5>√çndice</font></h2>\n",
    "\n",
    "<br>\n",
    "\n",
    "* [1. Introducci√≥n](#section1)\n",
    "* [2. Cambio de formato del DataSet](#section2)\n",
    "* [3. Modificaciones adicionales en DataSet](#section3)\n",
    "* [4. Creaci√≥n de caracter√≠sticas de playlist](#section4)\n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Permite establecer la anchura de la celda\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import emoji\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tqdm.notebook import tqdm as tqdm_nb\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar si es la primera vez que se emplea nltk\n",
    "\n",
    "#import nltk\n",
    "#nltk.download('punkt') \n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MPD_PATH = 'MPD'\n",
    "MPD_TEST_PATH = 'MPD_TEST'\n",
    "MPD_CSV_PATH = 'MPD_CSV'\n",
    "MPD_SLICE_PREFIX = 'mpd.slice.'\n",
    "\n",
    "ALBUMS_FILE = os.path.join(MPD_CSV_PATH,'mpd.albums.csv')\n",
    "ARTISTS_FILE = os.path.join(MPD_CSV_PATH,'mpd.artists.csv')\n",
    "TRACKS_FILE = os.path.join(MPD_CSV_PATH,'mpd.tracks.csv')\n",
    "PLSTRS_FILE = os.path.join(MPD_CSV_PATH,'mpd.pls-tracks.csv')\n",
    "PLSTARTS_FILE = os.path.join(MPD_CSV_PATH,'mpd.pls-artists.csv')\n",
    "PLSINFO_FILE = os.path.join(MPD_CSV_PATH,'mpd.playlists-info.csv')\n",
    "PLSTESTINFO_FILE = os.path.join(MPD_CSV_PATH,'mpd.playlists-info-test.csv')\n",
    "NAMES_FILE = os.path.join(MPD_CSV_PATH, \"mpd.names.csv\")\n",
    "PLSTRS_PREFIX = 'mpd.playlists-tracks.'\n",
    "EMOJI_DICT_FILE = os.path.join(MPD_CSV_PATH, \"emojis_translation_dict.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "<a id=\"section1\"></a>\n",
    "## <font color=\"#92002A\">1 - Introducci√≥n</font>\n",
    "<br>\n",
    "\n",
    "En esta libreta vamos a convertir el *dataset* que hemos obtenido previamente, en formato *JSON*, a formato *CSV*. Tambi√©n vamos a generar las caracter√≠sticas de las playlists para usar en el modelo de *LightFM* que crearemos m√°s adelante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">\n",
    "<a href=\"#indice\"><font size=5><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#92002A\"></i></font></a>\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section2\"></a>\n",
    "## <font color=\"#92002A\">2 - Cambio de formato del DataSet</font>\n",
    "<br>\n",
    "\n",
    "Cuando creamos los ficheros que contienen el dataset con el conjunto de playlists, empleamos el formato JSON y lo almacenamos en varios archivos comprimidos debido a su elevado tama√±o: 7,5 GB (43,3 GB sin comprimir). Tambi√©n nos encontramos con el caso de que tenemos la informaci√≥n repetida de las pistas, puesto que cada playlist contiene toda la informaci√≥n completa de √©stas.\n",
    "\n",
    "\n",
    "Para solucionar estos problemas y poder trabajar mejor con los datos, vamos a convertirlos al formato CSV, con lo que tendremos la informaci√≥n repartida en varias tablas y evitaremos tener informaci√≥n repetida de forma innecesaria. Se ha creado un fichero/tabla para cada tipo de item del conjunto:\n",
    "\n",
    "*\t√Ålbumes.\n",
    "*\tArtistas.\n",
    "*\tPistas.\n",
    "*\tInformaci√≥n sobre las playlists.\n",
    "*\tInformaci√≥n sobre las playlists del conjunto de prueba.\n",
    "*\tLista de pistas de las playlists (para ambos conjuntos).\n",
    "\n",
    "Adicionalmente, hemos creado una tabla que contiene la lista de artistas de las playlists, junto al n√∫mero de veces que aparece cada artista en ellas.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<a id=\"section21\"></a>\n",
    "### <font color=\"#B20033\">2.1 - Definici√≥n de funciones adicionales</font>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n que nos permite leer un archivo .json comprimido o sin comprimir\n",
    "# y devuelve un diccionario con su contenido\n",
    "def json_file_reader(file_path):\n",
    "    \"\"\"\n",
    "    :param file_path: Ruta del fichero a leer.\n",
    "    :return results: Diccionario con los datos leidos del fichero JSON.\n",
    "    \"\"\"\n",
    "    _ , file_extension = os.path.splitext(file_path)\n",
    "\n",
    "    # Fichero comprimido\n",
    "    if file_extension == '.zip':\n",
    "        with ZipFile(file_path,'r') as zip_file:\n",
    "            with zip_file.open(zip_file.namelist()[0]) as json_file:\n",
    "                json_data = json.load(json_file)\n",
    "    # Fichero sin comprimir\n",
    "    elif file_extension == '.json':\n",
    "        with open(file_path, \"r\") as json_file:\n",
    "            json_data = json.load(json_file)            \n",
    "    # En caso de que sea otra extensi√≥n, devolvemos un diccionario vac√≠o\n",
    "    else:\n",
    "        json_data = {}            \n",
    "    \n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n que se encarga de convertir el dataset que contiene \n",
    "# 1 mill√≥n de playlists a formato CSV\n",
    "def jsonds_to_csvds(json_ds_path,csv_ds_path):\n",
    "    \"\"\"\n",
    "    :param json_ds_path: Ruta donde se encuentra el conjunto de datos en formato JSON.\n",
    "    :param csv_ds_path: Ruta donde almacenaremos el nuevo conjunto de datos en formato CSV.\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(csv_ds_path):\n",
    "        os.mkdir(csv_ds_path)    \n",
    "    \n",
    "    files = []\n",
    "    tracks_dict = defaultdict(dict)\n",
    "\n",
    "    for file in os.listdir(json_ds_path):\n",
    "        if file.startswith(MPD_SLICE_PREFIX):\n",
    "            files.append(os.path.join(json_ds_path,file))\n",
    "\n",
    "    plstrs_fieldnames = ['pid','pos','track_uri']\n",
    "    tracks_fieldnames = ['track_name', 'track_uri', 'duration_ms', 'artist_name', \n",
    "                         'artist_uri', 'album_name', 'album_uri']\n",
    "    plsinfo_fieldnames = ['pid','name','collaborative','modified_at',\n",
    "                          'num_albums','num_tracks', 'num_followers',\n",
    "                          'num_edits','duration_ms','num_artists']\n",
    "\n",
    "    with open(PLSINFO_FILE,'w',newline='') as csv_file:\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=plsinfo_fieldnames,delimiter=',')\n",
    "        writer.writeheader()\n",
    "\n",
    "    for file in tqdm_nb(files):\n",
    "        file_name , _ = os.path.splitext(file)\n",
    "        portion = file_name.split('.')[-1]\n",
    "        csv_pltrs_file = os.path.join(csv_ds_path, f\"{PLSTRS_PREFIX}{portion}.csv\")\n",
    "        row_list = []\n",
    "\n",
    "        with open(PLSINFO_FILE,'a',encoding='utf8',newline='') as csv_file:\n",
    "            writer = csv.DictWriter(csv_file, fieldnames=plsinfo_fieldnames,\n",
    "                                    delimiter=',',quoting=csv.QUOTE_MINIMAL)\n",
    "            for pl in json_file_reader(file)['playlists']:\n",
    "                tracks_list = pl.pop('tracks')\n",
    "                writer.writerow(pl)\n",
    "                for track in tracks_list:\n",
    "                    pos = track.pop('pos')\n",
    "                    row = {'pid': pl['pid'], 'pos': pos,\n",
    "                           'track_uri' : track['track_uri']}\n",
    "                    row_list.append(row)\n",
    "                    tracks_dict[track['track_uri']] = track\n",
    "\n",
    "        with open(csv_pltrs_file,'w',newline='') as csv_tracks_file:\n",
    "            writer_tracks = csv.DictWriter(csv_tracks_file, fieldnames=plstrs_fieldnames)\n",
    "            writer_tracks.writeheader()\n",
    "            for row in row_list:\n",
    "                writer_tracks.writerow(row)\n",
    "    \n",
    "    print(\"Volcado de informaci√≥n sobre pistas:\")\n",
    "    with open(TRACKS_FILE,'w',newline='', encoding='utf8') as csv_tracks_file:\n",
    "        writer_tracks = csv.DictWriter(csv_tracks_file, fieldnames=tracks_fieldnames)\n",
    "        writer_tracks.writeheader()\n",
    "        pbar = tqdm_nb(total=len(tracks_dict))\n",
    "        for track in tracks_dict.values():\n",
    "            writer_tracks.writerow(track)\n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n que se encarga de convertir el conjunto de datos (test)\n",
    "# a formato CSV\n",
    "def jsonds_to_csvds_test(json_ds_path,csv_ds_path):\n",
    "    \"\"\"\n",
    "    :param json_ds_path: Ruta donde se encuentra el conjunto de datos en formato JSON.\n",
    "    :param csv_ds_path: Ruta donde almacenaremos el nuevo conjunto de datos en formato CSV.\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(csv_ds_path):\n",
    "        os.mkdir(csv_ds_path)\n",
    "\n",
    "    file_name = \"mpd.test.zip\"\n",
    "    \n",
    "    if file_name in os.listdir(json_ds_path):\n",
    "        plstrs_fieldnames = ['pid','pos','track_uri']\n",
    "        plsinfo_fieldnames = ['pid','name','num_holdouts','num_samples',\n",
    "                              'num_tracks']\n",
    "        row_list = []\n",
    "        \n",
    "        with open(PLSTESTINFO_FILE, 'w',newline='',encoding='utf8') as csv_file:\n",
    "            writer = csv.DictWriter(csv_file, fieldnames=plsinfo_fieldnames,\n",
    "                                    delimiter=',',quoting=csv.QUOTE_MINIMAL)\n",
    "            writer.writeheader()\n",
    "            test_playlists = json_file_reader(os.path.join(json_ds_path,file_name))['playlists']\n",
    "            for pl in test_playlists:\n",
    "                tracks_list = pl.pop('tracks')\n",
    "                writer.writerow(pl)\n",
    "                \n",
    "                for track in tracks_list:\n",
    "                    pos = track.pop('pos')\n",
    "                    row = {'pid': pl['pid'], 'pos': pos,\n",
    "                           'track_uri' : track['track_uri']}\n",
    "                    row_list.append(row)\n",
    "        \n",
    "        plstrs_test_path = os.path.join(MPD_CSV_PATH,\"{}test.csv\".format(PLSTRS_PREFIX))\n",
    "        with open(plstrs_test_path, 'w', newline='', encoding='utf8') as csv_tracks_file:\n",
    "            writer_tracks = csv.DictWriter(csv_tracks_file, fieldnames=plstrs_fieldnames)\n",
    "            writer_tracks.writeheader()\n",
    "            for row in row_list:\n",
    "                writer_tracks.writerow(row)    \n",
    "    else:\n",
    "        print(\"ERROR: Fichero del conjunto de prueba no encontrado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonds_to_csvds(MPD_PATH,MPD_CSV_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonds_to_csvds_test(MPD_TEST_PATH,MPD_CSV_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tras este proceso el conjunto queda convertido a formato *CSV*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">\n",
    "<a href=\"#indice\"><font size=5><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#92002A\"></i></font></a>\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section3\"></a>\n",
    "## <font color=\"#92002A\">3 - Modificaciones adicionales en DataSet</font>\n",
    "<br>\n",
    "\n",
    "Como los identificadores Spotify contienen unos ‚Äúprefijos‚Äù que resultan √∫nicos en el caso de los √°lbumes, artistas y pistas, los eliminamos (ya que en caso de ser necesario podemos volver a a√±adirlos). Ejemplo:\n",
    "\n",
    "_spotify:album:**0MlTOiC5ZYKFGeZ8h3D4rd**_ => *0MlTOiC5ZYKFGeZ8h3D4rd*\n",
    "\n",
    "\n",
    "Para que las tablas sean m√°s f√°ciles de entender y ocupen menos tama√±o, hemos establecido nuestro propio identificador, al que llamaremos PID, para √°lbumes, pistas y artistas. De esta manera las relaciones entre los elementos de las tablas, que hemos almacenado en ficheros CSV, se hacen empleando estos identificadores, y no por el c√≥digo alfanum√©rico de Spotify\n",
    "\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track_name</th>\n",
       "      <th>track_uri</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>artist_name</th>\n",
       "      <th>artist_uri</th>\n",
       "      <th>album_name</th>\n",
       "      <th>album_uri</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No Control</td>\n",
       "      <td>spotify:track:3G0MlTNw2AX3Xdbhrj33OS</td>\n",
       "      <td>237306</td>\n",
       "      <td>Holland</td>\n",
       "      <td>spotify:artist:21KpdYIquYJUEiEcPO2cnI</td>\n",
       "      <td>No Control</td>\n",
       "      <td>spotify:album:5F9BLbIkEvApFC8flNVKYR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>City Lights</td>\n",
       "      <td>spotify:track:3R9H16eUSv5vJ9DEgMG2Lu</td>\n",
       "      <td>369693</td>\n",
       "      <td>Holland</td>\n",
       "      <td>spotify:artist:21KpdYIquYJUEiEcPO2cnI</td>\n",
       "      <td>No Control</td>\n",
       "      <td>spotify:album:5F9BLbIkEvApFC8flNVKYR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sunshine</td>\n",
       "      <td>spotify:track:1YgaIKDgm7IQpq9MVAmExQ</td>\n",
       "      <td>252066</td>\n",
       "      <td>Keane</td>\n",
       "      <td>spotify:artist:53A0W3U0s8diEn9RhXQhVz</td>\n",
       "      <td>Hopes And Fears</td>\n",
       "      <td>spotify:album:0MlTOiC5ZYKFGeZ8h3D4rd</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    track_name                             track_uri  duration_ms artist_name  \\\n",
       "0   No Control  spotify:track:3G0MlTNw2AX3Xdbhrj33OS       237306     Holland   \n",
       "1  City Lights  spotify:track:3R9H16eUSv5vJ9DEgMG2Lu       369693     Holland   \n",
       "2     Sunshine  spotify:track:1YgaIKDgm7IQpq9MVAmExQ       252066       Keane   \n",
       "\n",
       "                              artist_uri       album_name  \\\n",
       "0  spotify:artist:21KpdYIquYJUEiEcPO2cnI       No Control   \n",
       "1  spotify:artist:21KpdYIquYJUEiEcPO2cnI       No Control   \n",
       "2  spotify:artist:53A0W3U0s8diEn9RhXQhVz  Hopes And Fears   \n",
       "\n",
       "                              album_uri  \n",
       "0  spotify:album:5F9BLbIkEvApFC8flNVKYR  \n",
       "1  spotify:album:5F9BLbIkEvApFC8flNVKYR  \n",
       "2  spotify:album:0MlTOiC5ZYKFGeZ8h3D4rd  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tracks = pd.read_csv(TRACKS_FILE)\n",
    "df_tracks.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_artists = df_tracks[['artist_name', 'artist_uri']].copy()\n",
    "df_artists.drop_duplicates(subset=['artist_uri'],inplace=True)\n",
    "df_artists.reset_index(drop=True, inplace=True)\n",
    "df_artists.index.name = 'artist_pid'\n",
    "df_artists['artist_id'] = df_artists['artist_uri'].apply(lambda x: x.split(':')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_albums = df_tracks[['album_name', 'album_uri', 'artist_uri']].copy()\n",
    "df_albums.drop_duplicates(subset=['album_uri'],inplace=True)\n",
    "df_albums.reset_index(drop=True, inplace=True)\n",
    "df_albums.index.name = 'album_pid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tracks.drop(columns=['artist_name','album_name'],inplace=True)\n",
    "df_tracks.drop_duplicates(subset='track_uri',inplace=True)\n",
    "df_tracks.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tracks = pd.merge(df_tracks, df_artists.reset_index()[['artist_pid','artist_uri']], on=['artist_uri']) \\\n",
    "                .drop(columns=['artist_uri'])\n",
    "df_tracks = pd.merge(df_tracks, df_albums.reset_index()[['album_pid','album_uri']], on=['album_uri']) \\\n",
    "                .drop(columns=['album_uri'])\n",
    "\n",
    "df_tracks.index.name = 'track_pid'\n",
    "\n",
    "df_tracks['track_id'] = df_tracks['track_uri'].apply(lambda x: x.split(':')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_albums = pd.merge(df_albums, df_artists.reset_index()[['artist_pid','artist_uri']], on=['artist_uri']) \\\n",
    "                .drop(columns=['artist_uri'])\n",
    "df_albums.index.name = 'album_pid'\n",
    "\n",
    "df_albums['album_id'] = df_albums['album_uri'].apply(lambda x: x.split(':')[-1])\n",
    "df_albums.drop(columns=['album_uri'], inplace = True)\n",
    "df_artists.drop(columns=['artist_uri'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n empleada para leer un dataframe que ha sido almacenado\n",
    "# en varios ficheros\n",
    "def read_dataset_multifile(ds_prefix, folder=os.curdir):\n",
    "    \"\"\"\n",
    "    :param ds_prefix: Prefijo de los ficheros a leer.\n",
    "    :param folder: Directorio donde se encuentran los ficheros.\n",
    "    :return: Dataframe resultante de leer los ficheros.\n",
    "    \"\"\"\n",
    "    list_df = []\n",
    "    \n",
    "    for file_name in os.listdir(folder):\n",
    "        if file_name.startswith(ds_prefix):\n",
    "            file_path = os.path.join(folder, file_name)\n",
    "            df_temp = pd.read_csv(file_path)\n",
    "            list_df.append(df_temp)\n",
    "            \n",
    "    return pd.concat(list_df, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plstrs = read_dataset_multifile(PLSTRS_PREFIX,MPD_CSV_PATH)\n",
    "\n",
    "trackid_map_dict = df_tracks[['track_uri']].to_dict()['track_uri']\n",
    "trackid_map_dict = {v: k for k, v in trackid_map_dict.items()}\n",
    "\n",
    "df_plstrs[\"track_pid\"] = df_plstrs[\"track_uri\"].map(trackid_map_dict)\n",
    "df_plstrs.drop(columns=['track_uri'], inplace=True)\n",
    "df_tracks.drop(columns=['track_uri'], inplace=True)\n",
    "df_plstrs.sort_values(['pid', 'pos'],inplace=True)\n",
    "df_plstrs.set_index('pid',inplace=True)\n",
    "df_plstrs.index.name = 'pl_pid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for file_name in os.listdir(MPD_CSV_PATH):\n",
    "    if file_name.startswith(PLSTRS_PREFIX):\n",
    "        os.remove(os.path.join(MPD_CSV_PATH,file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plsarts = pd.merge(df_plstrs.reset_index(), \n",
    "                      df_tracks.reset_index()[['track_pid', 'artist_pid']], on=['track_pid'])\n",
    "df_plsarts.drop(columns=['track_pid','pos'], inplace=True)\n",
    "df_plsarts.sort_values(by=['pl_pid','artist_pid'], inplace=True)\n",
    "df_plsarts = df_plsarts.groupby(['pl_pid','artist_pid']).size().to_frame(name = 'artist_count').reset_index()\n",
    "df_plsarts.set_index('pl_pid',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "plinfo_dtypes = {'modified_at' : int, 'num_albums': int, 'num_tracks': int, 'num_followers' : int,\n",
    "                 'num_edits': int, 'duration_ms' : int, 'num_artists': int}\n",
    "\n",
    "df_plsinfo = pd.read_csv(PLSINFO_FILE, dtype=plinfo_dtypes,index_col=0)\n",
    "df_plsinfo.index.name = 'pl_pid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "plinfo_test_dtypes = {'num_holdouts' : int, 'num_samples': int, 'num_tracks': int}\n",
    "\n",
    "df_plsinfo_test = pd.read_csv(PLSTESTINFO_FILE, dtype=plinfo_test_dtypes, index_col=0)\n",
    "df_plsinfo_test.index.name = 'pl_pid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_artists.to_csv(ARTISTS_FILE)\n",
    "df_albums.to_csv(ALBUMS_FILE)\n",
    "df_tracks.to_csv(TRACKS_FILE)\n",
    "df_plsinfo.to_csv(PLSINFO_FILE)\n",
    "df_plsinfo_test.to_csv(PLSTESTINFO_FILE)\n",
    "df_plstrs.to_csv(PLSTRS_FILE)\n",
    "df_plsarts.to_csv(PLSTARTS_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">\n",
    "<a href=\"#indice\"><font size=5><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#92002A\"></i></font></a>\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section4\"></a>\n",
    "## <font color=\"#92002A\">4 - Creaci√≥n de caracter√≠sticas de playlist</font>\n",
    "<br>\n",
    "\n",
    "A continuaci√≥n, vamos a convertir los t√≠tulos en etiquetas, en las que cada una corresponder√° a una caracter√≠stica.\n",
    "\n",
    "Como podemos recordar, algunas playlists contienen emoticonos. Para este caso creamos un diccionario con el cual cambiar dichos elementos por palabras.\n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section21\"></a>\n",
    "### <font color=\"#B20033\">Emoticonos a texto</font>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprueba si un car√°cter es un emoticono\n",
    "def is_emoji(char):\n",
    "    return char in emoji.UNICODE_EMOJI['en']\n",
    "\n",
    "# Obtiene la lista de emoticonos que aparecen en un texto dado\n",
    "def get_emojis_list(text):\n",
    "    text_chars = list(text)\n",
    "    emojis_set = set()    \n",
    "    for char in text_chars:\n",
    "        if is_emoji(char):\n",
    "            emojis_set.add(char)            \n",
    "    return list(emojis_set)\n",
    "\n",
    "# Devuelve un string con los emoticonos que aparecen en un texto dado\n",
    "def get_emojis_string(text):\n",
    "    emojis_list = get_emojis_list(text)\n",
    "    return \"\".join(emojis_list)\n",
    "\n",
    "# Comprueba si un texto tiene emoticonos\n",
    "def has_emojis(text):\n",
    "    return len(get_emojis_list(text)) > 0\n",
    "\n",
    "# Elimina los emoticonos de un texto\n",
    "def remove_emojis(text):\n",
    "    return emoji.get_emoji_regexp().sub(u'', text)\n",
    "\n",
    "# De una lista de cadenas, devuelve aquellas que contienen\n",
    "# el emoticono indicado\n",
    "def get_names_with_emoji(emj, names):\n",
    "    names_list = []\n",
    "    \n",
    "    if is_emoji(emj):\n",
    "        for name in names:\n",
    "            if emj in name:\n",
    "                names_list.append(name)\n",
    "    else:\n",
    "        raise Exception(\"'{}' no es un emoji.\".format(emj))\n",
    "        \n",
    "    return names_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_frequent_words(names):\n",
    "    words_count_dict = defaultdict(int)\n",
    "    \n",
    "    for name in names:\n",
    "        words = name.split(' ')\n",
    "        for word in words:\n",
    "            # Comprobamos si la palabra no es vacia y no contiene un d√≠gito\n",
    "            if word != '' and not any(str.isdigit(c) for c in word):\n",
    "                words_count_dict[word.lower()] += 1\n",
    "            \n",
    "    # Ordenamos las tuplas de palabras por su n√∫mero de apariciones\n",
    "    words_count_dict = sorted(words_count_dict.items(), \n",
    "                              key=lambda k_v: k_v[1], \n",
    "                              reverse=True)\n",
    "    \n",
    "    # Extraemos en una lista la palabra de cada tupla\n",
    "    words_list = [word for (word,count) in words_count_dict]\n",
    "            \n",
    "    return words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emoji_most_frequent_words(emoj, names_list, num_words=-1):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    names = get_names_with_emoji(emoj, names_list)\n",
    "    for i, name in enumerate(names):\n",
    "        clean_name = re.sub(r'[^\\w\\s]','',remove_emojis(name).lower()).strip()\n",
    "        clean_name_tokenized = list()\n",
    "        for token in word_tokenize(clean_name):\n",
    "            if lemmatizer.lemmatize(token) not in set(stopwords.words(\"english\")):\n",
    "                clean_name_tokenized.append(lemmatizer.lemmatize(token))\n",
    "        names[i] = ' '.join(clean_name_tokenized)\n",
    "    word_list = most_frequent_words(names)[0:num_words]\n",
    "    \n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_emoji_dict(names_list, removable_words=[], num_tags=5):\n",
    "    names_with_emoji_list = [name for name in names_list if has_emojis(name)]\n",
    "    \n",
    "    emojis_set = set()    \n",
    "    for name in names_with_emoji_list:\n",
    "        emojis_set.update(get_emojis_list(name))    \n",
    "    \n",
    "    emoji_dict = dict()\n",
    "    for emoj in tqdm_nb(emojis_set):\n",
    "        emoji_dict[emoj] = emoji_most_frequent_words(emoj, names_with_emoji_list)\n",
    "        \n",
    "    words_count_dict = defaultdict(int)\n",
    "    \n",
    "    for _,value in emoji_dict.items():\n",
    "        for element in value:\n",
    "            words_count_dict[element] +=1\n",
    "    \n",
    "    words_count_dict = sorted(words_count_dict.items(), \n",
    "                              key=lambda k_v: k_v[1], \n",
    "                              reverse=True)\n",
    "            \n",
    "    for k,v in emoji_dict.items():\n",
    "        emoji_dict[k] = [word for word in v if word not in removable_words][0:num_tags]\n",
    "        \n",
    "    emoji_dict = {k: v for k, v in emoji_dict.items() if len(v) > 0}\n",
    "    \n",
    "    return emoji_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "<a id=\"section31\"></a>\n",
    "### <font color=\"#B20033\">Limpieza de texto</font>\n",
    "\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation_chars(text):\n",
    "    unwanted_chars = string.punctuation.replace(\"'\", \"\")\n",
    "    if any(x in text for x in string.punctuation):\n",
    "        for c in unwanted_chars :\n",
    "            text = text.replace(c, ' ')\n",
    "            \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_multiple_whitespaces(text):\n",
    "    return re.sub(' +', ' ', text).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = remove_emojis(text.lower())\n",
    "    text = remove_punctuation_chars(text)\n",
    "    text = remove_multiple_whitespaces(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "<a id=\"section31\"></a>\n",
    "### <font color=\"#B20033\">Creaci√≥n de etiquetas</font>\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tags(cleaned_text, emojis_list=[], emojis_translation_dict=dict(), max_emoji_translations=3):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokenized_text = set(word_tokenize(cleaned_text))\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    text_tags_set = set(stemmer.stem(word) for word in tokenized_text \n",
    "                        if word not in stop_words)\n",
    "\n",
    "    if len(emojis_list) > 0 and len(emojis_translation_dict) > 0:\n",
    "        emojis_tags_set = set()\n",
    "        for emoj in emojis_list:\n",
    "            if emoj in emoji_dict:\n",
    "                emojis_tags_set.update(emojis_translation_dict[emoj][0:max_emoji_translations])\n",
    "        \n",
    "        text_tags_set = text_tags_set.union(emojis_tags_set)\n",
    "    \n",
    "    return \"|\".join(list(text_tags_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unusual_tags(name_tags_list,min_appearances=2):\n",
    "    tags_count = defaultdict(int)\n",
    "    for name_tags in name_tags_list:\n",
    "        for tag in name_tags.split('|'):\n",
    "            tags_count[tag] += 1\n",
    "\n",
    "    removable_set = set()\n",
    "    for tag,count in tags_count.items():\n",
    "        if count < min_appearances:\n",
    "            removable_set.add(tag)\n",
    "            \n",
    "    return removable_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unusual_tags(tags, unusual_tags_set):\n",
    "    tags_list = tags.split('|')\n",
    "    new_tags_list = []\n",
    "    for tag in tags_list:\n",
    "        if tag not in unusual_tags_set:\n",
    "            new_tags_list.append(tag)\n",
    "            \n",
    "    return ('|').join(new_tags_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "<a id=\"section31\"></a>\n",
    "### <font color=\"#B20033\">Proceso final</font>\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_names_df(df_names, emojis_translation_dict=dict(), remove_unusualtags=False):\n",
    "    total_steps = 3\n",
    "    if remove_unusualtags:\n",
    "        total_steps += 1\n",
    "    \n",
    "    tqdm_nb.pandas()\n",
    "    print(\"(1/{}) Cleaning playlists names\".format(total_steps))\n",
    "    df_names['clean_name'] = df_names.name.progress_apply(clean_text)\n",
    "    print(\"(2/{}) Emojis translation\".format(total_steps))\n",
    "    df_names['emojis'] = df_names.name.progress_apply(get_emojis_string)\n",
    "    print(\"(3/{}) Generating tags\".format(total_steps))\n",
    "    df_names['tags'] = df_names.progress_apply(lambda df: get_tags(df['clean_name'], df['emojis'],\n",
    "                                                                   emojis_translation_dict,\n",
    "                                                                   max_emoji_translations=3),axis=1)\n",
    "    \n",
    "    df_names.drop(columns=['emojis'], inplace=True)\n",
    "    df_names.drop(columns=['clean_name'], inplace=True)\n",
    "    \n",
    "    if remove_unusualtags:\n",
    "        print(\"(4/{}) Removing unusual tags\".format(total_steps))\n",
    "        removable_tags_set = get_unusual_tags(df_names.tags.to_list()) \n",
    "        df_names['tags'] = df_names.progress_apply(lambda df: remove_unusual_tags(df['tags'], \n",
    "                                                                                  removable_tags_set),\n",
    "                                                   axis=1)    \n",
    "    \n",
    "    return df_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f74864d06d494b24a6dc3c5cfe3294ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1081 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Diccionario de emoticonos\n",
    "if os.path.isfile(EMOJI_DICT_FILE):\n",
    "    with open(EMOJI_DICT_FILE) as json_file:\n",
    "        emoji_dict = json.load(json_file)\n",
    "else:\n",
    "    names = pd.read_csv(PLSINFO_FILE, index_col=0)['name'].astype(str).to_list()\n",
    "    names = names + pd.read_csv(PLSTESTINFO_FILE, index_col=0)['name'].astype(str).to_list()\n",
    "    emoji_dict = create_emoji_dict(names, removable_words= [\"music\", \"playlist\", \"song\"])\n",
    "    with open(EMOJI_DICT_FILE, 'w') as json_file:\n",
    "        json.dump(emoji_dict, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('üéê', ['bit', 'everything', 'swav√©', 'mi', 'watercolour']),\n",
       " ('üôÉ', ['sad', 'mood', 'love', 'feel', 'good']),\n",
       " ('‚ôë', ['capricorn', 'gamzee', 'makara', 'ive', 'hellbent']),\n",
       " ('üå§', ['morning', 'cloud', 'day', 'summer', 'spring']),\n",
       " ('‚ô£', ['attack', 'team', 'nigga', 'explosion', 'later']),\n",
       " ('ü§∂', ['christmas', 'justin', 'bieber', 'drummer']),\n",
       " ('üöÇ', ['train', 'soul', 'express', 'toot', 'gain']),\n",
       " ('ü§∞', ['pregnant', 'word'])]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(emoji_dict.items())[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1/3) Cleaning playlists names\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b23aa2693b944ed18351c76ba0ee549d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1010000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2/3) Emojis translation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fc205c5521b44a281337b8fc7256575",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1010000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3/3) Generating tags\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46d39e21abff4bd1a0dbc480afc7c936",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1010000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# DataFrame nombres\n",
    "if os.path.isfile(NAMES_FILE):\n",
    "    df_names = pd.read_csv(NAMES_FILE, index_col=0)\n",
    "else:\n",
    "    df_names = pd.read_csv(PLSINFO_FILE, index_col=0)[['name']].astype(str)\n",
    "    df_names_test = pd.read_csv(PLSTESTINFO_FILE, index_col=0)[['name']].astype(str)\n",
    "    df_names = create_names_df(pd.concat([df_names,df_names_test]), emoji_dict, remove_unusualtags=False)\n",
    "    df_names.to_csv(NAMES_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pl_pid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Low viscosity vibes</td>\n",
       "      <td>viscos|low|vibe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dalanda üêâ</td>\n",
       "      <td>dalanda|imagine|game|dragon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>freeze pops</td>\n",
       "      <td>pop|freez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Golden Oldies</td>\n",
       "      <td>oldi|golden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I d‚óªn't g‚óªv‚óª a d‚óªmn</td>\n",
       "      <td>dmn|n't|dnt|gv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       name                         tags\n",
       "pl_pid                                                  \n",
       "0       Low viscosity vibes              viscos|low|vibe\n",
       "1                 dalanda üêâ  dalanda|imagine|game|dragon\n",
       "2               freeze pops                    pop|freez\n",
       "3             Golden Oldies                  oldi|golden\n",
       "4       I d‚óªn't g‚óªv‚óª a d‚óªmn               dmn|n't|dnt|gv"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_names.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">\n",
    "<a href=\"#indice\"><font size=5><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#92002A\"></i></font></a>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"text-align: right\"> <font size=6><i class=\"fa fa-graduation-cap\" aria-hidden=\"true\" style=\"color:#92002A\"></i> </font></div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
